wandb_version: 1

_current_progress_remaining:
  desc: null
  value: 1
_custom_logger:
  desc: null
  value: 'False'
_episode_num:
  desc: null
  value: 0
_last_episode_starts:
  desc: null
  value: "[ True  True  True  True  True  True  True  True  True  True  True  True\n\
    \  True  True  True  True  True  True  True  True  True  True  True  True\n  True\
    \  True  True  True  True  True  True  True  True  True  True  True\n  True  True\
    \  True  True  True  True  True  True  True  True  True  True\n  True  True  True\
    \  True  True  True  True  True  True  True  True  True\n  True  True  True  True]"
_last_obs:
  desc: null
  value: "[[-0.0991274  -0.1652087   0.20559181 ...  1.          1.\n   1.       \
    \ ]\n [-0.0991274  -0.1652087   0.20559181 ...  1.          1.\n   1.        ]\n\
    \ [-0.0991274  -0.1652087   0.20559181 ...  1.          1.\n   1.        ]\n ...\n\
    \ [-0.0991274  -0.1652087   0.20559181 ...  1.          1.\n   1.        ]\n [-0.0991274\
    \  -0.1652087   0.20559181 ...  1.          1.\n   1.        ]\n [-0.0991274 \
    \ -0.1652087   0.20559181 ...  1.          1.\n   1.        ]]"
_last_original_obs:
  desc: null
  value: None
_logger:
  desc: null
  value: <stable_baselines3.common.logger.Logger object at 0x7f057c2854f0>
_n_updates:
  desc: null
  value: 0
_total_timesteps:
  desc: null
  value: 50000000
_vec_normalize_env:
  desc: null
  value: None
_wandb:
  desc: null
  value:
    cli_version: 0.13.4
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.10
    start_time: 1668276523.787355
    t:
      1:
      - 1
      - 30
      - 50
      - 55
      2:
      - 1
      - 30
      - 50
      - 55
      3:
      - 13
      - 16
      - 22
      - 23
      - 35
      4: 3.8.10
      5: 0.13.4
      8:
      - 5
action_noise:
  desc: null
  value: None
action_space:
  desc: null
  value: Box(-1.0, 1.0, (30,), float32)
agent:
  desc: null
  value:
    multi_proc: true
    name: PPO
    params:
      batch_size: 256
      clip_range: 0.2
      ent_coef: 0.001
      gae_lambda: 0.95
      gamma: 0.95
      learning_rate: 1.0e-05
      n_epochs: 5
      n_steps: 4096
      vf_coef: 0.5
      warm_start_mean: true
    policy_kwargs:
      log_std_init: -1.6
      net_arch:
      - pi:
        - 512
        - 256
        - 128
        vf:
        - 512
        - 256
        - 128
algo:
  desc: null
  value: PPO
batch_size:
  desc: null
  value: 256
checkpoints:
  desc: null
  value:
    name_prefix: rl_model
    save_freq: 4000000
    save_path: ./models/
clip_range:
  desc: null
  value: <function constant_fn.<locals>.func at 0x7f05da3f6550>
clip_range_vf:
  desc: null
  value: None
defaults:
  desc: null
  value:
    agent: ppo
    env: pgdm
device:
  desc: null
  value: cuda
ent_coef:
  desc: null
  value: 0.001
env:
  desc: null
  value:
    env_kwargs: {}
    info_keywords:
    - obj_err
    - obj_success
    - step_obj_err
    - time_frac
    - obj_err_scale
    n_envs: ${n_envs}
    name: toothpaste-lift
    state_keyword: state
    task_kwargs:
      append_time: true
      pregrasp: motion_planned
      reward_kwargs:
        lift_bonus_mag: 2.5
        lift_bonus_thresh: 0.02
        n_envs: ${n_envs}
        obj_com_term: 0.25
        obj_err_scale: 50
        obj_reward_ramp: 0
        obj_reward_start: 0
        object_reward_scale: 10.0
    vid_freq: ${vid_freq}
    vid_length: 100
ep_info_buffer:
  desc: null
  value: deque([], maxlen=100)
ep_success_buffer:
  desc: null
  value: deque([], maxlen=100)
eval_env:
  desc: null
  value: None
eval_freq:
  desc: null
  value: 1000000
exp_name:
  desc: null
  value: MimicTrainer
gae_lambda:
  desc: null
  value: 0.95
gamma:
  desc: null
  value: 0.95
id:
  desc: null
  value: ${hydra.job.id}
learning_rate:
  desc: null
  value: 1.0e-05
lr_schedule:
  desc: null
  value: <function constant_fn.<locals>.func at 0x7f05da4395e0>
max_grad_norm:
  desc: null
  value: 0.5
n_envs:
  desc: null
  value: 64
n_epochs:
  desc: null
  value: 5
n_eval_envs:
  desc: null
  value: 5
n_steps:
  desc: null
  value: 64
num_timesteps:
  desc: null
  value: 0
observation_space:
  desc: null
  value: Box(-inf, inf, (322,), float32)
policy:
  desc: null
  value: "ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten):\
    \ Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n  \
    \  (shared_net): Sequential()\n    (policy_net): Sequential(\n      (0): Linear(in_features=322,\
    \ out_features=512, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=512,\
    \ out_features=256, bias=True)\n      (3): Tanh()\n      (4): Linear(in_features=256,\
    \ out_features=128, bias=True)\n      (5): Tanh()\n    )\n    (value_net): Sequential(\n\
    \      (0): Linear(in_features=322, out_features=512, bias=True)\n      (1): Tanh()\n\
    \      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): Tanh()\n\
    \      (4): Linear(in_features=256, out_features=128, bias=True)\n      (5): Tanh()\n\
    \    )\n  )\n  (action_net): Linear(in_features=128, out_features=30, bias=True)\n\
    \  (value_net): Linear(in_features=128, out_features=1, bias=True)\n)"
policy_class:
  desc: null
  value: <class 'tcdm.rl.models.policies.ActorCriticPolicy'>
policy_kwargs:
  desc: null
  value: '{''net_arch'': [{''pi'': [512, 256, 128], ''vf'': [512, 256, 128]}], ''log_std_init'':
    -1.6}'
restore_checkpoint_freq:
  desc: null
  value: 500000
resume_model:
  desc: null
  value: null
rollout_buffer:
  desc: null
  value: <stable_baselines3.common.buffers.RolloutBuffer object at 0x7f040a9d0100>
save_freq:
  desc: null
  value: 10000000
sde_sample_freq:
  desc: null
  value: -1
seed:
  desc: null
  value: 0
start_time:
  desc: null
  value: 1668276569.6025229
target_kl:
  desc: null
  value: None
tensorboard_log:
  desc: null
  value: logs/
total_timesteps:
  desc: null
  value: 50000000
use_sde:
  desc: null
  value: 'False'
verbose:
  desc: null
  value: 1
vf_coef:
  desc: null
  value: 0.5
vid_freq:
  desc: null
  value: null
wandb:
  desc: null
  value:
    project: PGDM
    run_name: ${env.name}_${now:%Y-%m-%d}
    sweep_name_prefix: run
